name: edge_cloud_independent_ppo_fc_depth2
# https://docs.ray.io/en/master/rllib-algorithms.html
# rllib algorithms and their configs
# common hyperparameters: https://docs.ray.io/en/master/rllib-training.html#common-parameters

env: edge_cloud

env-config:
  n_nodes: 3
  num_agents: 3
  # agents should learn that low-value tasks should not be allocated to node 0
  avg_resource_capacity: { 0: [ 50, 50, 50 ], 1: [ 3, 3, 3 ], 2: [ 3, 3, 3 ] }
  avg_unit_cost: { 0: [ 0, 0, 0 ], 1: [ 30, 30, 30 ], 2: [ 30, 30, 30 ] }
#  verbose: True
  verbose: False
#  "log_level": "DEBUG"
  obs_length: 58
  n_actions: 2


trainer: PPO
trainer-config:
  framework: torch
  num_envs_per_worker: 1
  # number of workers for collecting samples with.
  num_workers: 5
  num_gpus: 0
  # set a seed_value value for each worker as well as for the environment
  seed: 123

  # Model config
  model:
    custom_model: FCModel
    custom_model_config:
      layers: [256, 256]

  # Trainer parameters
  lr:
#    gridsearch: [0.01, 0.001, 0.0001]
    gridsearch: [0.0001]
  lambda: 1.0
  entropy_coeff: 0.0
  batch_mode: "complete_episodes"
#  rollout_fragment_length: 220
  # The size of each SGD epoch = Size of Training Set
  train_batch_size:
    gridsearch: [4000]
  # The minibatch size within each epoch.
  # This number of samples will be worked through before updating the model's
  #    internal parameters.
#  sgd_minibatch_size: 128


stop:
  timesteps_total: 1_000_000

#samples: 1
