name: DroneRescue gridworld_radar_vision_net_ppo

env: gridworld_radar_vision_net
env-config:
  width: 50
  height: 50
  num_survivors: 20
  num_radar_drones: 2
  num_rescue_drones: 2
  start_world: [ [ ] ]
  sight: 5
  battery: 500
  rewards:
    rescue: 10
    hit tree: 0
    exploring: 0
    mark agent: 1
  battery costs:
    rotate left: 1
    rotate right: 1
    advance: 2
  fire spread:
    starting points: 0
    covariance: [ [ 3, 0 ], [ 0, 3 ] ]
    rate: 0.1
  autogen config:
    forest fire:
      chance: 1
      trees:
        scale: 20.0
        octaves: 8
        persistence: 0.5
        lacunarity: 2.0
        threshold: 0.07
      rocks:
        scale: 6.0
        octaves: 10
        persistence: 0.5
        lacunarity: 5.0
        threshold: 0.20
      hq:
        size: 5


trainer: PPO
trainer-config:
  framework: torch
  num_envs_per_worker: 1
  num_workers: 10
  num_gpus: 1

  # Model config
  model:
    dim: 11
    conv_filters: [ [ 16, [ 3, 3 ], 2 ], [ 32, [ 3, 3 ], 2 ], [ 512, [ 3, 3 ], 1 ] ]
#    use_lstm: true
    # To further customize the LSTM auto-wrapper
#    "lstm_cell_size": 64

  # Trainer parameters
  lr:
    loguniform: [0.00001, 0.001]
  lambda:
    uniform: [0.9, 1]
  gamma: 0.99
  rollout_fragment_length: 100
  train_batch_size: 5000
  sgd_minibatch_size: 500
  entropy_coeff: 0.01


# Use population based trainer
scheduler: pbt
scheduler-config:
  time_attr: training_iteration
  perturbation_interval: 5
  metric: episode_reward_mean
  mode: max

  hyperparam_mutations:
    lr:
      loguniform: [0.00001, 0.001]
    lambda:
      uniform: [0.9, 1]
    gamma:
      uniform: [0.95, 1]

    rollout_fragment_length: [ 20, 100, 200 ]
    train_batch_size: [ 4000, 5000 ]
    sgd_minibatch_size: [ 128, 500 ]

    observation_filter: [ MeanStdFilter, NoFilter ]

stop:
  timesteps_total: 1_000_000

grouping: radar-rescue
