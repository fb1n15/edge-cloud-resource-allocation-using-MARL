name: 5-actions_first-price_with-history_revenue
# https://docs.ray.io/en/master/rllib-algorithms.html
# rllib algorithms and their configs
# common hyperparameters: https://docs.ray.io/en/master/rllib-training.html#common-parameters

#env: edge_cloud
env: edge_cloud1

env-config:
  n_nodes: 3
  num_agents: 3
  n_tasks_in_total: 42
  n_tasks_to_allocate: 40
  duration: 7
  usage_time_ub: 3
  seed: 124
  random_seed: True

# agents should learn that low-value tasks should not be allocated to node 0
  avg_resource_capacity: { 0: [ 6, 6, 6], 1: [6, 6, 6], 2:[4, 4, 4]}
  avg_unit_cost: { 0: [ 2, 2, 2], 1:[2, 2, 2], 2:[5, 5, 5]}
#  verbose: True
  verbose: False
#  "log_level": "DEBUG"
  record_history: True
  history_len: 4
  cooperative: False
  n_actions: 5
  auction_type: 'first-price'
#  auction_type: 'second-price'


trainer: PPO
trainer-config:
  framework: torch
  # number of workers for collecting samples with.
  num_workers: 7 # use 6 cores out of 8 CPU cores
  num_gpus: 0
#  # set a seed_value value for each worker as well as for the environment
  seed: 124
  # Element-wise observation filter, either "NoFilter" or "MeanStdFilter".
  # https://gitmemory.com/issue/ray-project/ray/9399/683323479
  # https://intellabs.github.io/coach/components/filters/input_filters.html
  "observation_filter": "MeanStdFilter"
  # Whether to synchronize the statistics of remote filters.
  "synchronize_filters": True

  # Model config
  model:
    custom_model: FCModel
    custom_model_config:
      layers:
        gridsearch: [[256, 256]]
#        gridsearch: [[256, 256], [256, 256, 256]]
    # Share layers for value function. If you set this to True, it's
    # important to tune vf_loss_coeff.
    "vf_share_layers": True

  # Trainer parameters
  lr:
#     gridsearch: [0.001]
    gridsearch: [0.0001, 0.001]
  lambda: # GAE parameter
    gridsearch: [1.0, 0.9]
  kl_coeff: # initial coefficient for KL divergence
    gridsearch: [0.3, 0.6]
  rollout_fragment_length: 200 # size of batches collected from each worker
  # The size of each SGD epoch = Size of the Training Set
  train_batch_size:
    gridsearch: [3000]
  # The minibatch size within each epoch.
  # This number of samples will be worked through before updating the model's
  #    internal parameters.
  sgd_minibatch_size:
    gridsearch: [128, 256]

  # Coefficient of the value function loss. IMPORTANT: you must tune this if
  # you set vf_share_layers=True inside your model's config.
  "vf_loss_coeff":
    gridsearch: [0.5, 0.8]
  entropy_coeff: # coefficient of the entropy regularizer
    gridsearch: [0, 0.01]
  clip_param: # PPO Clipping parameter.
    gridsearch: [0.1, 0.2]
  vf_clip_param: # Clip parameter for the value function.
    gridsearch: [0.2]
#  grad_clip: # Clip the gradient.
#    gridsearch: None

#  kl_target: # Target KL divergence to aim for.
#    gridsearch: [0.01, 0.02]

  # whether to rollout "complete_episodes" or "truncate_episodes"
  batch_mode: "complete_episodes"
#  rollout_fragment_length: 220
#    gridsearch: [3000, 4000]


stop:
#  timesteps_total: 10_0000
  timesteps_total: 5_0000

# how many trials to run for each hyperparameter combinations
samples: 1
